{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Cell 2: Model hyperparameters\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Cell 3: Transformer Block\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        #   Multi-head attention layer\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        \n",
    "        #   Feed-forward network layer: two dense layers with ReLU activation\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "\n",
    "        #   Layer normalization layers\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        #   Dropout layers\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        #   Self-attention\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "\n",
    "        #   Layer normalization\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        #   Feed-forward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "\n",
    "        #   Layer normalization\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Cell 4: Token and Position Embedding\n",
    "@keras.saving.register_keras_serializable()\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # maxlen = ops.shape(x)[-1]\n",
    "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_size = 'full' #   Use the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Cell 6: Functions to make predictions on manually provided text\n",
    "\n",
    "def manual_predict(man_text, model):\n",
    "    try: \n",
    "        vec_text = vectorize_layer(tf.constant([man_text]))\n",
    "        return model.predict(vec_text, verbose=0)\n",
    "    except:\n",
    "        print(f'Prediction failed on {man_text}')\n",
    "        return None\n",
    "\n",
    "def manual_odds(man_text, model):\n",
    "    result = manual_predict(man_text, model)\n",
    "    if result is None:\n",
    "        return None\n",
    "    else:\n",
    "        return result.tolist()[0][0]\n",
    "\n",
    "# to be filled in with our appropriate labels\n",
    "def manual_bin(man_text):\n",
    "    if manual_odds(man_text) >= 0.5:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Cell 7: Model Testing Function\n",
    "def test_model(test_set, model):\n",
    "    df['predicted_odds'] = df['raw_text'].apply(lambda text: manual_odds(text, model))\n",
    "    df['prediction'] = df['predicted_odds'].apply(lambda x: 'TA' if x >= 0.5 else 'NTA')\n",
    "    df['is_correct'] = df['prediction'] == df['correct']\n",
    "    return len(df[df['is_correct']]) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_file(file):\n",
    "    try:\n",
    "        with open(file, 'r') as in_file:\n",
    "            text = in_file.read()\n",
    "        return text\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Cell 9: Load Training Dataset\n",
    "batch_size = 32\n",
    "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"../data_formatted/unbalanced/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Cell 10: Custom Standardization Function (same as training)\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "\n",
    "max_features = 2000\n",
    "embedding_dim = 64\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "text_ds = raw_train_ds.map(lambda x, y: x) #   Only extract text, not labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds)  #   Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_files = [f'../data_formatted/balanced/test/pos/{file}' for file in os.listdir('../data_formatted/balanced/test/pos')]\n",
    "neg_files = [f'../data_formatted/balanced/test/neg/{file}' for file in os.listdir('../data_formatted/balanced/test/neg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df = pd.DataFrame({\n",
    "    'file': neg_files,\n",
    "    'correct': 'NTA'\n",
    "})\n",
    "\n",
    "pos_df = pd.DataFrame({\n",
    "    'file': pos_files,\n",
    "    'correct': 'TA'\n",
    "})\n",
    "\n",
    "df = pd.concat([neg_df, pos_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['raw_text'] = [read_file(file) for file in df['file']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_models = [f'../model/{model}' for model in os.listdir('../model') if not 'transformer' in model]\n",
    "\n",
    "model_stats = pd.DataFrame({\n",
    "    'model': neural_models,\n",
    "    'accuracy_rate': [0] * len(neural_models)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Cell 16: Evaluate all models under the model directory\n",
    "for i in range(0, len(model_stats)):\n",
    "    curr_model = model_stats['model'][i]\n",
    "\n",
    "    model = keras.models.load_model(curr_model)\n",
    "    accuracy_rate = test_model(df, model)\n",
    "    print(f'Model {curr_model} has accuracy {accuracy_rate}')\n",
    "    model_stats.loc[i, 'accuracy_rate'] = accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats.sort_values('accuracy_rate', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats.to_csv(f'../model-stats_test-set-{test_set_size}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
