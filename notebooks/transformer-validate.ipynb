{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatgpt suggested function, see saved chat\n",
    "def dataset_to_numpy(dataset):\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for x_batch, y_batch in dataset:\n",
    "        xs.extend(x_batch.numpy())\n",
    "        ys.extend(y_batch.numpy())\n",
    "\n",
    "    return (\n",
    "        np.array(xs, dtype=object),\n",
    "        np.array(ys)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Text Preprocessing Functions\n",
    "#   Custom text standardization for preprocessing. \n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        #   Multi-head attention layer\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        #   Feed forward network layer\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        #   Layer normalization layers\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        #   Dropout layers\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token and positional embedding class\n",
    "@keras.saving.register_keras_serializable()\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # maxlen = ops.shape(x)[-1]\n",
    "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to manually predict given a specific text\n",
    "\n",
    "def manual_predict(man_text, model):\n",
    "    try: \n",
    "        vec_text = vectorize_layer(tf.constant([man_text]))\n",
    "        return model.predict(vec_text, verbose=0)\n",
    "    except:\n",
    "        print(f'Prediction failed on {man_text}')\n",
    "        return None\n",
    "\n",
    "def manual_odds(man_text, model):\n",
    "    result = manual_predict(man_text, model)\n",
    "    if result is None:\n",
    "        return None\n",
    "    else:\n",
    "        return result.tolist()[0][0]\n",
    "\n",
    "# to be filled in with our appropriate labels\n",
    "def manual_bin(man_text):\n",
    "    if manual_odds(man_text) >= 0.5:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model on test set that has been loaded into the df dataframe\n",
    "def test_model(test_set, model):\n",
    "    df['predicted_odds'] = df['raw_text'].apply(lambda text: manual_odds(text, model))\n",
    "    df['prediction'] = df['predicted_odds'].apply(lambda x: 'TA' if x >= 0.5 else 'NTA')\n",
    "    df['is_correct'] = df['prediction'] == df['correct']\n",
    "    return len(df[df['is_correct']]) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    try:\n",
    "        with open(file, 'r') as in_file:\n",
    "            text = in_file.read()\n",
    "        return text\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 382046 files belonging to 2 classes.\n",
      "Using 305637 files for training.\n",
      "Using 76409 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# prepare the vectorization step\n",
    "\n",
    "batch_size = 32\n",
    "raw_train_ds, raw_test_ds = keras.utils.text_dataset_from_directory(\n",
    "        \"../data_transformer/unbalanced\",\n",
    "        batch_size=batch_size,\n",
    "        seed=1337,\n",
    "        subset=\"both\",\n",
    "        validation_split=0.2,\n",
    "        labels=\"inferred\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 10:06:59.049582: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "embedding_dim = 128\n",
    "sequence_length = 200\n",
    "\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 10:07:12.595862: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = dataset_to_numpy(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = dataset_to_numpy(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "\n",
    "train_x = keras.utils.pad_sequences(train_x, maxlen=maxlen)\n",
    "test_x = keras.utils.pad_sequences(test_x, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean the test data\n",
    "\n",
    "pos_files = [f'../data_formatted/balanced/test/pos/{file}' for file in os.listdir('../data_formatted/balanced/test/pos')]\n",
    "neg_files = [f'../data_formatted/balanced/test/neg/{file}' for file in os.listdir('../data_formatted/balanced/test/neg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df = pd.DataFrame({\n",
    "    'file': neg_files,\n",
    "    'correct': 'NTA'\n",
    "})\n",
    "\n",
    "pos_df = pd.DataFrame({\n",
    "    'file': pos_files,\n",
    "    'correct': 'TA'\n",
    "})\n",
    "\n",
    "df = pd.concat([neg_df, pos_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['raw_text'] = [read_file(file) for file in df['file']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick validation\n",
    "# remove for full validation\n",
    "sample_rows = random.sample(list(df.index), test_set_size)\n",
    "df = df.iloc[sample_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [f'../model/{model}' for model in os.listdir('../model') if 'transformer' in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats = pd.DataFrame({\n",
    "    'model': models,\n",
    "    'accuracy_rate': [0] * len(models)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/declanbradley/.pyenv/versions/3.11.8/lib/python3.11/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'token_and_position_embedding_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/Users/declanbradley/.pyenv/versions/3.11.8/lib/python3.11/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'transformer_block_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction failed on None\n",
      "Model ../model/transformer_balanced_3-epochs.keras has accuracy 0.49598700554175423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7d/nwvg_sj134x6bj58g27_zxp40000gn/T/ipykernel_17916/2329327805.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.49598700554175423' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  model_stats.loc[i, 'accuracy_rate'] = accuracy_rate\n",
      "/Users/declanbradley/.pyenv/versions/3.11.8/lib/python3.11/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'token_and_position_embedding_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/Users/declanbradley/.pyenv/versions/3.11.8/lib/python3.11/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'transformer_block_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction failed on For context I was in a band, (I was given the cold shoulder because of this and left) it consisted of three guys I didn't know to well, me and another girl who I knew from school bands we had been in together. She was a singer and they wanted something that would make them stand out (I play the flute), so they asked me to kinda just see if I fit. I said sure but a classic sting instrument would be easier to incorporate with a wind.  Long story short I'm in and it's okay we all get along. (Music wasn't great but not the issue.) One day the singer I'll call T came up to me and said, \"you know it makes me feel really insecure about my chest when you wear things like that.\" I wear corsets or corset like tops or a regular top but with my waist trainer which honestly looks great, lots of chains or tight skirts and pants. Its just how I dress normally. Now I know I have very large beasts and I'm the opposite of a petite woman so the waist training helped me start to feel feminine and accentuates my feminine qualities that I didn't know I could have before. Now I am sure my execution was wrong as I'm not well spoken and often don't realize what I say can be taken another way than what I mean. I was very confused on what she wanted me to say as she just stood there and I was still holding my flute up to continue practicing. So I said, \"OK, how is that my problem?\" I was genuinely asking I wasn't snarky when I said it I didn't give attitude. She walked away in a huff and later told everyone how I was a bitch. Suddenly I was berated behind my back for being a cold bitch to the girl brave enough to share an insecurity. I was barely talked to during practice and one of the guys girlfriends. (She didn't like T for other reasons) told me I was because T told everyone how mean I was to her and cried infront of everyone telling them I was the reason she's been so depressed and self conscious of her own body. T has the exact opposite of my body type. Super petite, naturally skinny, beautiful tan skin, (I'm sickly pale due to a skin condition that doesn't allow me to be in the sun for long so am very jealousof her completion admittedly)and she's a very cute girl everyone has thought so since we were in school. I am just stummed on what the proper response would have been because in my anger I told bandmates girlfriend \"what did she want me to say? Sorry T, I'll start wearing strictly sweatpants and big baggy hoodies so you don't have to see my slutty body.\" I was told when she insulted me she said. \"Op just thinks everyone wants to look at her slutty body, but it's just gross.\" The only for word quote I got. So reddit I think I already know Iam the A for my response to T in the moment after writing this but tell me anyway. AITA?\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(model_stats)):\n",
    "    curr_model = model_stats['model'][i]\n",
    "\n",
    "    model = keras.models.load_model(curr_model)\n",
    "    accuracy_rate = test_model(df, model)\n",
    "    print(f'Model {curr_model} has accuracy {accuracy_rate}')\n",
    "    model_stats.loc[i, 'accuracy_rate'] = accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../model/transformer_balanced_3-epochs.keras</td>\n",
       "      <td>0.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>../model/transformer_balanced_6-epochs.keras</td>\n",
       "      <td>0.504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>../model/transformer_balanced_12-epochs.keras</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../model/transformer_balanced_10-epochs.keras</td>\n",
       "      <td>0.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../model/transformer_unbalanced_6-epochs.keras</td>\n",
       "      <td>0.483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../model/transformer_balanced_20-epochs.keras</td>\n",
       "      <td>0.483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>../model/transformer_unbalanced_3-epochs.keras</td>\n",
       "      <td>0.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../model/transformer_unbalanced_10-epochs.keras</td>\n",
       "      <td>0.466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>../model/transformer_unbalanced_12-epochs.keras</td>\n",
       "      <td>0.465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             model  accuracy_rate\n",
       "0     ../model/transformer_balanced_3-epochs.keras          0.515\n",
       "8     ../model/transformer_balanced_6-epochs.keras          0.504\n",
       "5    ../model/transformer_balanced_12-epochs.keras          0.500\n",
       "2    ../model/transformer_balanced_10-epochs.keras          0.492\n",
       "1   ../model/transformer_unbalanced_6-epochs.keras          0.483\n",
       "4    ../model/transformer_balanced_20-epochs.keras          0.483\n",
       "6   ../model/transformer_unbalanced_3-epochs.keras          0.476\n",
       "3  ../model/transformer_unbalanced_10-epochs.keras          0.466\n",
       "7  ../model/transformer_unbalanced_12-epochs.keras          0.465"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stats.sort_values('accuracy_rate', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats.to_csv(f'../validation-stats/transformer-stats_test-set-{test_set_size}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
